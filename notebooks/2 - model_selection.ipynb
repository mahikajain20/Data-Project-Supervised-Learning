{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should include preliminary and baseline modeling.\n",
    "- Try as many different models as possible.\n",
    "- Don't worry about hyperparameter tuning or cross validation here.\n",
    "- Ideas include:\n",
    "    - linear regression\n",
    "    - support vector machines\n",
    "    - random forest\n",
    "    - xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Disable all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Import from functions_variables.py\n",
    "from functions_variables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.read_csv('../data/dataframes/X_train_encoded.csv')\n",
    "# y = pd.read_csv('../data/dataframes/Y_train_encoded.csv')\n",
    "#Not encoding for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../data/dataframes/X_train.csv')\n",
    "y = pd.read_csv('../data/dataframes/Y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop permalink, listing, primary_photo, state, postal code\n",
    "X.drop(columns=['permalink', 'listing', 'primary_photo', 'state', 'postal_code', 'city', 'sold_date'], inplace=True)\n",
    "#Drop date and city too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save X to csv again \n",
    "X.to_csv('../data/dataframes/X_train_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the encoded data into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3204, 23)\n",
      "(801, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best thing to not encode in MVP \n",
    "and then try later if u have brains for it \n",
    "\n",
    "use state? \n",
    "using static dict or dynamic encoding? \n",
    "\n",
    "no Y should be indicated in Kfold train data \n",
    "data leakage - should not have access to the future prices\n",
    "(me knowing what I am going to do in the future)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save again \n",
    "X_train.to_csv('../data/dataframes/X_train_final2.csv', index=False)\n",
    "X_val.to_csv('../data/dataframes/X_val_final.csv', index=False)\n",
    "Y_train.to_csv('../data/dataframes/Y_train_final.csv', index=False)\n",
    "Y_val.to_csv('../data/dataframes/Y_val_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models \n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'Elastic Net': ElasticNet(random_state=42),\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42),\n",
    "    'Support Vector Machine': SVR(),\n",
    "    'XGBoost': xgb.XGBRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what metrics you want to use to evaluate success.\n",
    "- If you think about mean squared error, can we actually relate to the amount of error?\n",
    "- Try root mean squared error so that error is closer to the original units (dollars)\n",
    "- What does RMSE do to outliers?\n",
    "- Is mean absolute error a good metric for this problem?\n",
    "- What about R^2? Adjusted R^2?\n",
    "- Briefly describe your reasons for picking the metrics you use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE): This is the average of the squared differences between the predicted and actual values. It's a popular metric for regression problems, but its value can be hard to interpret because it's in squared units.\n",
    "\n",
    "Root Mean Squared Error (RMSE): This is the square root of the MSE. It's in the same units as the target variable, which can make it easier to interpret than MSE. It gives a higher weight to larger errors, which can be useful if large errors are particularly undesirable.\n",
    "\n",
    "Mean Absolute Error (MAE): This is the average of the absolute differences between the predicted and actual values. It's less sensitive to outliers than MSE and RMSE.\n",
    "\n",
    "R^2: This is the coefficient of determination, which measures how well the predicted values fit the actual values. A value of 1 means a perfect fit. It's a popular metric for regression problems, but it can be misleading if the model is overfitting.\n",
    "\n",
    "Adjusted R^2: This is a modified version of R^2 that takes into account the number of predictors in the model. It's generally a better choice than R^2 when comparing models with different numbers of predictors.\n",
    "\n",
    "If your data has many outliers, MAE might be a good choice because it's less sensitive to outliers than MSE and RMSE. If you care more about large errors, RMSE might be a good choice. If you want a balance between these two, you might consider using both RMSE and MAE.\n",
    "\n",
    "R^2 or adjusted R^2 can be useful for getting a sense of how well the model fits the data overall, but they should be used in conjunction with other metrics, not in isolation.\n",
    "\n",
    "Remember to consider the nature of your problem and your specific needs when choosing metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R2: 0.29348543043156283\n",
      "Linear Regression MSE: 216346981985.07208\n",
      "Linear Regression RMSE: 465131.14493126783\n",
      "Linear Regression MAE: 195268.97186896513\n",
      "\n",
      "\n",
      "Decision Tree R2: 0.9935170009893199\n",
      "Decision Tree MSE: 1985206435.3458178\n",
      "Decision Tree RMSE: 44555.65548104772\n",
      "Decision Tree MAE: 7110.639200998751\n",
      "\n",
      "\n",
      "Random Forest R2: 0.9719021853398747\n",
      "Random Forest MSE: 8604036864.812567\n",
      "Random Forest RMSE: 92757.94771777008\n",
      "Random Forest MAE: 23025.069375780276\n",
      "\n",
      "\n",
      "K-Nearest Neighbors R2: 0.759734546103638\n",
      "K-Nearest Neighbors MSE: 73573437922.87675\n",
      "K-Nearest Neighbors RMSE: 271244.24034968327\n",
      "K-Nearest Neighbors MAE: 137401.28164794008\n",
      "\n",
      "\n",
      "Gradient Boosting R2: 0.9212704295458354\n",
      "Gradient Boosting MSE: 24108356280.81085\n",
      "Gradient Boosting RMSE: 155268.65839830926\n",
      "Gradient Boosting MAE: 94444.86698563586\n",
      "\n",
      "\n",
      "AdaBoost R2: 0.7059626175984804\n",
      "AdaBoost MSE: 90039332539.4789\n",
      "AdaBoost RMSE: 300065.54707176716\n",
      "AdaBoost MAE: 252481.59546373616\n",
      "\n",
      "\n",
      "Elastic Net R2: 0.2883285232603424\n",
      "Elastic Net MSE: 217926116161.38812\n",
      "Elastic Net RMSE: 466825.5735940225\n",
      "Elastic Net MAE: 178525.50836489382\n",
      "\n",
      "\n",
      "Ridge R2: 0.29353120835199964\n",
      "Ridge MSE: 216332963994.00067\n",
      "Ridge RMSE: 465116.07582839\n",
      "Ridge MAE: 195194.91095449103\n",
      "\n",
      "\n",
      "Lasso R2: 0.2934875231562798\n",
      "Lasso MSE: 216346341156.5653\n",
      "Lasso RMSE: 465130.4560621303\n",
      "Lasso MAE: 195265.9283024109\n",
      "\n",
      "\n",
      "Support Vector Machine R2: -0.023266035022033726\n",
      "Support Vector Machine MSE: 313341759647.0442\n",
      "Support Vector Machine RMSE: 559769.3807694775\n",
      "Support Vector Machine MAE: 208771.80803284334\n",
      "\n",
      "\n",
      "XGBoost R2: 0.9898067712783813\n",
      "XGBoost MSE: 3121348464.797754\n",
      "XGBoost RMSE: 55869.02956735291\n",
      "XGBoost MAE: 13922.399310432273\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Model', 'R2', 'MSE', 'RMSE', 'MAE']) #initialize dataframe\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    result = train_models(name, model, X_train, Y_train, X_val, Y_val) #Function from functions_variables.py\n",
    "    results.loc[i] = result #Add results to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R2</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.993517</td>\n",
       "      <td>1.985206e+09</td>\n",
       "      <td>44555.655481</td>\n",
       "      <td>7110.639201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.989807</td>\n",
       "      <td>3.121348e+09</td>\n",
       "      <td>55869.029567</td>\n",
       "      <td>13922.399310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.971902</td>\n",
       "      <td>8.604037e+09</td>\n",
       "      <td>92757.947718</td>\n",
       "      <td>23025.069376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.921270</td>\n",
       "      <td>2.410836e+10</td>\n",
       "      <td>155268.658398</td>\n",
       "      <td>94444.866986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.759735</td>\n",
       "      <td>7.357344e+10</td>\n",
       "      <td>271244.240350</td>\n",
       "      <td>137401.281648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.705963</td>\n",
       "      <td>9.003933e+10</td>\n",
       "      <td>300065.547072</td>\n",
       "      <td>252481.595464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.293531</td>\n",
       "      <td>2.163330e+11</td>\n",
       "      <td>465116.075828</td>\n",
       "      <td>195194.910954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.293488</td>\n",
       "      <td>2.163463e+11</td>\n",
       "      <td>465130.456062</td>\n",
       "      <td>195265.928302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.293485</td>\n",
       "      <td>2.163470e+11</td>\n",
       "      <td>465131.144931</td>\n",
       "      <td>195268.971869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>0.288329</td>\n",
       "      <td>2.179261e+11</td>\n",
       "      <td>466825.573594</td>\n",
       "      <td>178525.508365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>-0.023266</td>\n",
       "      <td>3.133418e+11</td>\n",
       "      <td>559769.380769</td>\n",
       "      <td>208771.808033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model        R2           MSE           RMSE  \\\n",
       "1            Decision Tree  0.993517  1.985206e+09   44555.655481   \n",
       "10                 XGBoost  0.989807  3.121348e+09   55869.029567   \n",
       "2            Random Forest  0.971902  8.604037e+09   92757.947718   \n",
       "4        Gradient Boosting  0.921270  2.410836e+10  155268.658398   \n",
       "3      K-Nearest Neighbors  0.759735  7.357344e+10  271244.240350   \n",
       "5                 AdaBoost  0.705963  9.003933e+10  300065.547072   \n",
       "7                    Ridge  0.293531  2.163330e+11  465116.075828   \n",
       "8                    Lasso  0.293488  2.163463e+11  465130.456062   \n",
       "0        Linear Regression  0.293485  2.163470e+11  465131.144931   \n",
       "6              Elastic Net  0.288329  2.179261e+11  466825.573594   \n",
       "9   Support Vector Machine -0.023266  3.133418e+11  559769.380769   \n",
       "\n",
       "              MAE  \n",
       "1     7110.639201  \n",
       "10   13922.399310  \n",
       "2    23025.069376  \n",
       "4    94444.866986  \n",
       "3   137401.281648  \n",
       "5   252481.595464  \n",
       "7   195194.910954  \n",
       "8   195265.928302  \n",
       "0   195268.971869  \n",
       "6   178525.508365  \n",
       "9   208771.808033  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('R2', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather evaluation metrics and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees\n",
    "\n",
    "A decision tree is a type of supervised learning algorithm that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on the most significant splitter/differentiator in input variables.\n",
    "\n",
    "Decision Trees: Decision trees can handle both numerical and categorical data, which is common in housing data. For example, a house's neighborhood is categorical, while its size is numerical. Decision trees can also model nonlinear relationships, which can be common in housing data.However, decision trees can be prone to overfitting, especially if they are allowed to become very deep. This can lead to poor generalization performance on new data.\n",
    "\n",
    "XGBoost stands for eXtreme Gradient Boosting. Rather than training all the models in isolation of one another, boosting trains models in succession, with each new model being trained to correct the errors made by the previous ones.\n",
    "XGBoost is a powerful, flexible, and efficient version of the gradient boosting algorithm. It can handle both numerical and categorical data, and it can model complex nonlinear relationships. XGBoost also has built-in regularization to prevent overfitting, which can lead to better generalization performance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRETCH**\n",
    "\n",
    "Even with all the preprocessing we did in Notebook 1, you probably still have a lot of features. Are they all important for prediction?\n",
    "\n",
    "Investigate some feature selection algorithms (Lasso, RFE, Forward/Backward Selection)\n",
    "- Perform feature selection to get a reduced subset of your original features\n",
    "- Refit your models with this reduced dimensionality - how does performance change on your chosen metrics?\n",
    "- Based on this, should you include feature selection in your final pipeline? Explain\n",
    "\n",
    "Remember, feature selection often doesn't directly improve performance, but if performance remains the same, a simpler model is often preferrable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature selection \n",
    "# refit models\n",
    "# gather evaluation metrics and compare to the previous step (full feature set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
